{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pluralsight ML Coding Challenge\n",
    "Daniel Stack, dstack1776@gmail.com \n",
    "\n",
    "github handle: dstack1776\n",
    "\n",
    "June 25, 2018\n",
    "\n",
    "# Instructions\n",
    "\n",
    "There are two separate Python files. They should be put in the same directory. The first of these is \"Pluralsight Class Modules\". I have provided it both as a Jupyter Notebook and as a standard Python file. The function main makes a number of calls to parse the csv files, calculate similarity, and create a sqlite database \"dstack.db\" that stores the results in the table \"distance\\_matrix\", with three columns -- src\\_usr, dst\\_usr, and distance. This takes several minutes to complete - there's some optimizations I'd consider (and will discuss below) were this not a model but rather a much larger dataset.\n",
    "\n",
    "The second file is rest\\_dstack.py that serves as a RESTful API interface. Like the first program, it is a toy with basic functionality. In this case is is listening to 127.0.0.1:5002. There are two get commands, users and usersn, detailed below.\n",
    "\n",
    "The folder \"data\\_files\\_ml\\_engineer\" should be within the folder with these Python files. (i.e. as a sub-folder of the cloned Pluralsight-ML.)\n",
    "\n",
    "## users\n",
    "\"users\" has a single argument, the user handle, which ranges from 1 to 10000, inclusive. It returns a dictionary of the ten nearest user handles (not including itself) and the distance, which ranges from 0 (identical) to 1 (nothing in common). For example, 127.0.0.1:5002/users/9999 will return the ten nearest handles to user handle 9999 as well as the distances from 9999. I ran this program straight out of IDLE. It needs to be run after \"Pluralsight Class Modules\" as it does a simple sql select from dstack.db.\n",
    "\n",
    "Example output for 127.0.0.1:5002/users/9999\n",
    "\n",
    "```python\n",
    "{\"1562\": 0.0, \"5238\": 0.0, \"6303\": 0.0, \"4484\": 0.5, \"4432\": 0.5196152422706632, \"906\": 0.5204164998665332, \"2200\": 0.5443310539518175, \"3605\": 0.5446711546122731, \"5071\": 0.5608545472127794, \"453\": 0.5773502691896258}\n",
    "```\n",
    "\n",
    "## usersn\n",
    "\"usersn\" has two arguments, id and num. id is used for the user handle while num indicates how many handles should be returned, in ascending order, starting with the closest.\n",
    "\n",
    "Example output for http://127.0.0.1:5002/usersn?id=9999&num=15\n",
    "```python\n",
    "{\"1562\": 0.0, \"5238\": 0.0, \"6303\": 0.0, \"4484\": 0.5, \"4432\": 0.5196152422706632, \"906\": 0.5204164998665332, \"2200\": 0.5443310539518175, \"3605\": 0.5446711546122731, \"5071\": 0.5608545472127794, \"453\": 0.5773502691896258, \"637\": 0.5773502691896258, \"2754\": 0.5773502691896258, \"5411\": 0.5773502691896258, \"6188\": 0.5773502691896258, \"7688\": 0.5773502691896258}\n",
    "```\n",
    "\n",
    "\n",
    "# Similarity Calculation\n",
    "## Aborted Effort\n",
    "When I first looked into this problem, I had initially considered using cosine similarity for the User Assessments. With all of the assessments having numeric values it seemed a reasonable mechanism. However, this entailed creating a column for every assessment type and the code to an **extremely** long time to run, even with this limited dataset. Moreover, it became clear that even were the performance issues to be overcome, it was not a good model. There were users who took no assessments - which made them identical to other users who took no assessments, as I needed to give them some numeric value (0). In the end, this did not appear a good model to use.\n",
    "\n",
    "I can think of some models where I would be able to make use of cosine similarity with missing values. For example, when rating movies, one could have a simple -1, 0, +1 rating. A 0 in such an example might work reasonably well for neutral or no opinion. Netflix has such a rating system - you can give a movie a thumb's up or a thumb's down. This does not work well with assessments - a 0 score is very different from not taking the assessment.\n",
    "\n",
    "## Three-Axis Jaccard Distance\n",
    "Instead, I made use of three axes of Jaccard distance. I set up three axes of distance -- for assessments, classes examined, and self-described interests.\n",
    "\n",
    "Jaccard similarity is used to compare sets. It is the cardinality of the intersection of two sets divided by the union of the two sets. I added some code to make the similarity between empty sets as 0 (so as to avoid divide by zero error). Given the most similar (identical) sets would have a 1 under this model, I subtracted the result from 1 to get a distance.\n",
    "\n",
    "I settled on making use of a set compare given the wide variety of interests, courses, and assessments. This avoided the many hoops I ran into when attempting to make use of cosine similarity.\n",
    "\n",
    "### Assessments\n",
    "One thing I did not want to lose on assessments were the scores. I wanted similarity between people who took assessments in a given area to have some similarity but I wanted even greater similarity if their levels of experience were similar. \n",
    "\n",
    "To facilitate this, I added three possible tags to every assessment - \\_Novice, \\_Proficient, and \\_Expert. Someone who tested as a Novice in Python, for example, would have the entry 'Python\\_Novice' added to their assessment set. Someone who tested as an Expert in Python have 'Python\\_Novice', 'Python\\_Proficient', and 'Python\\_Expert' added to their assessment set. In this model, two Python Experts would have a strong level of similarity given they would have three matching entries in their sets. Similarly a Python Novice and Python Expert would still have some level of similarity. I used benchmarks of 100 and 200 to show Proficiency and Expertise, aligning with Pluralsight's testing algorithms.\n",
    "\n",
    "### Interests\n",
    "Interests were the most straightforward as they were simple binaries - a user was either interested in something or not. I added interests into a user's Interest set.\n",
    "\n",
    "### Classes\n",
    "Given the variety of courses, I took advantage of the course tags to simplify areas of interest into broader categories - for each user, instead of using the course name I instead mapped in the tags from the separate csv file, using logic akin to a SQL left join. Like the Assessments, I took advantage of the difficulty of the course. I did not use the time spent on a class's webpage, an area of potential enhancement - perhaps using it to measure an intensity of interest - or to possibly discard entries that a user spent an extremely brief amount of time on.\n",
    "\n",
    "## Combining the Three Axes\n",
    "Since I calculated three separate distance axes, I combined them using a simple application of the Pythagorean Theorem. To keep the result scaled from 0 to 1, I divided this result by the square root of 3. \n",
    "\n",
    "\n",
    "# Considerations at Larger Scale\n",
    "\n",
    "I was able to take advantage of storing a lot of data in memory. Indeed, some of my initial efforts involved building a Jaccard Distance table for everything. It would have been possible to go with this model and not used a database table at all. However, the database table has as an advantage it can be built over time and in pieces. For example, I built the SQL table for one user at a time, comparing it with all potential other users in one 1D array. This was a large data structure - at a larger scale it might be reasonable (or even necessary) to build it in smaller chunks. However, given in my initial experiments I was able to store a 10,000 x 10,000 matrix, I believe that if it were necessary, it would be possible to store the results of one user's comparison with every other user for fairly large data sets.\n",
    "\n",
    "That said, the performance of the initial loading of sql table was mediocre. It takes several minutes to complete. If this is an operation which will run daily, that is possibly a reasonable time, even were we to be dealing with larger databases. I did do the SQL writes one at a time. I experimented with only committing when the operations were complete but that did not make a noticeable improvement. I would consider using transactions to speed up these operations - http://bytes.schibsted.com/speeding-up-sqlite-insert-operations/.\n",
    "\n",
    "To speed up calculations it is possible to take advantage of the distance between a source and destination is commutative - you only need to calculate it once, though this would add to the complexity of potential sql queries to retrieve the data - or to do a pair of sql writes, flipping the source and destination. \n",
    "\n",
    "Though parallel processing is not the answer for all performance problems, this would appear to be an excellent opportunity to take advantage of any such capacity. The number of users can be divided equally between the  processors with no to minimal coordination required between them during processing.\n",
    "\n",
    "scikit-learn has a number of similarity algorithms -- including one for Jaccard similarity. I decided to write my own basic version. This is definitely an area where experimentation, both in home-brew and in open source/BSD/etc. solutions, would be profitable.\n",
    "\n",
    "I did some experimentation between an explicit loop, going through every row and doing unions and intersections between every row one at a time as opposed to performing them all at once via a lambda operation (which of course is doing its own loop). The lambda gave slightly better performance.\n",
    "\n",
    "There is also the option of dispensing with the three-axis model. I made use of as a way to group similar characteristics together, giving equal weight to similarity in all three categories. It would be another valid option to have put everything in one big set.\n",
    "\n",
    "# Other Thoughts\n",
    "\n",
    "One thing I would have liked would have been some information as to what sorts of assessments, class tags, and interests are similar to one another. Domain knowledge could have been used to make such decisions, likely codifying it with a CSV file similar to the classes' tag file.  For example, Pluralsight's webpage breaks the assessments into Development, IT Ops, Data, Security, Creative categories. These may have been data points appropriate for similarity calculations.\n",
    "\n",
    "The REST interface is fairly basic and likely could be improved. This was my first exercise at creating a REST interface - since on my interview with Connor I spoke how I have talent at developing new skills, it seemed an absolutely fair challenge for me to illustrate my capacity to do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import array\n",
    "import sqlite3\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distances_array(in_array, num_users):\n",
    "    \"\"\"\n",
    "    Create an array of Jaccard distances for N users by N users - in the sample set this would\n",
    "    be 10,000 by 10,000.  In the end I did not use this method for my final design but am\n",
    "    maintaining it for the time being.\n",
    "    \n",
    "    Inputs - \n",
    "    in_array - a 1D array w/ an entry per user. The entry is a set for Jaccard distance calcualtion.\n",
    "    num_users - Number of users\n",
    "    \n",
    "    Returns - A Jaccard distance array\n",
    "    \"\"\"\n",
    "\n",
    "    distance_sets = np.full((num_users, num_users),1.0)\n",
    "\n",
    "    for src_index, src_row in enumerate(distance_sets):\n",
    "\n",
    "        for dst_index, dst_row in enumerate(in_array):\n",
    "            # Do a triangle and mirror\n",
    "            if src_index > dst_index:\n",
    "                continue\n",
    "            src_row = in_array[src_index]\n",
    "            dst_row = in_array[dst_index]\n",
    "            if src_row and dst_row:\n",
    "                # If either set is empty no point continuing.  Otherwise get cardinalities\n",
    "                intersection_cardinality = len(set.intersection(*[set(src_row), set(dst_row)]))\n",
    "                union_cardinality = len(set.union(*[set(src_row), set(dst_row)]))\n",
    "                distance_sets[src_index][dst_index] = 1.0 - (\n",
    "                    float(intersection_cardinality)/float(union_cardinality))\n",
    "                \n",
    "    for src_index, src_row in enumerate(distance_sets):\n",
    "        for dst_index, dst_row in enumerate(distance_sets):\n",
    "            # Mirror half of the triangle\n",
    "            if src_index > dst_index:\n",
    "                distance_sets[src_index][dst_index] = distance_sets[dst_index][src_index]\n",
    "        print(distance_sets[src_index])\n",
    "    return distance_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jaccard_distances_user(in_array, user_handle, num_users):\n",
    "    \"\"\"\n",
    "    For a given user_handle, calculate the distance to all other users. \n",
    "    \n",
    "    Inputs - \n",
    "    in_array - a 1D array w/ an entry per user. The entry is a set for Jaccard distance calcualtion.\n",
    "    user_handle - 1-based user handle\n",
    "    num_users - Number of users\n",
    "    \n",
    "    Returns - A Jaccard distance array for the given user handle   \n",
    "    \"\"\"\n",
    "    \n",
    "    distance_sets = np.full((num_users),1.0) # Initialize all distances to 1 (max)\n",
    "    src_index = user_handle - 1 # Array is 0-based, user handle is 1-based\n",
    "    src_row = in_array[src_index]\n",
    "\n",
    "    union_cnt = list(map(lambda x: len(set.union(x, src_row)), in_array))\n",
    "    intersect_cnt = list(map(lambda x: len(set.intersection(x, src_row)), in_array))\n",
    "    \n",
    "    union_cnt = np.array(union_cnt)\n",
    "    intersect_cnt = np.array(intersect_cnt)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        # Potential divide by zero if the union is an empty set\n",
    "        # The similarity is the cardinality of the intersection divided by that of the union.\n",
    "        # That result is then subtracted from 1 to determine the distance.\n",
    "        distance_sets = 1.0 - np.nan_to_num(intersect_cnt/union_cnt)\n",
    "    return distance_sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_3_axis_distances_for_one_handle(Assmnts_Obj, Ints_Obj, Cls_Obj, user_handle):\n",
    "    \"\"\"\n",
    "    For a given user_handle, calculate the distance to all other users across all three axes. \n",
    "    Assumes Assessments_Obj, Interests_Obj, and Classes_Obj are global. Performs multiple \n",
    "    Jaccard Distance caluclations and then uses the Pythagorean Theorem to combine them. \n",
    "    Write results to SQLite table.\n",
    "    \n",
    "    Inputs - \n",
    "    Assmnts_Obj - A member of the Assessments class\n",
    "    Ints_Obj - A member of the Interests class\n",
    "    Cls_Obj - A member of the Classes class\n",
    "    user_handle - 1-based user handle\n",
    "    \n",
    "    Returns - Nothing. Results written to SQLite table.   \n",
    "    \"\"\"\n",
    "\n",
    "    # Note user-handle is 1-based.  The functions being called are aware of that.\n",
    "    assessment_distance = Assmnts_Obj.calculate_handle_jaccard_distances(user_handle)\n",
    "    interests_distance = Ints_Obj.calculate_handle_jaccard_distances(user_handle)\n",
    "    classes_distance = Cls_Obj.calculate_handle_jaccard_distances(user_handle)\n",
    "    \n",
    "    overall_distance = (np.sqrt((assessment_distance ** 2) + (interests_distance ** 2) + \n",
    "                                (classes_distance ** 2))) / np.sqrt(3)\n",
    "    \n",
    "    conn = sqlite3.connect('dstack.db')\n",
    "  \n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # Write to sql table\n",
    "    for idx, entry in enumerate(overall_distance):\n",
    "        c.execute(\"\"\"INSERT INTO distance_matrix VALUES (?, ?, ?)\"\"\", \n",
    "                  (user_handle, idx + 1, overall_distance[idx],))\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Commit the changes\n",
    "    conn.commit()\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PS_Data:\n",
    "    \"\"\"\n",
    "    Parent class for Assessments, Intersts, and Classes. Stores dataframe for corresponding csv\n",
    "    file as well as user_data_sets which stores sets that indicate Assessments, Intersts, and\n",
    "    Classes for all users. \n",
    "    \"\"\"\n",
    "    def __init__(self, input_file):\n",
    "        self.input_file = input_file # Text string indicating csv file\n",
    "        self.data = None # Dataframe to hold csv file contents\n",
    "        self.user_list = None # List of all known users\n",
    "        self.num_users = 0 # Number of users\n",
    "        self.user_data_sets = None # Array of sets w/ user assessments, classes, or interests\n",
    "        self.jaccard_distances = None # If used, an array of all Jaccard distances b/t users\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Read csv file into a pandas dataframe.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.data = pd.read_csv(self.input_file)\n",
    "        except FileNotFoundError:\n",
    "            print('load_data - could not find file {0}'.format(self.input_file))\n",
    "            sys.exit() # cease execution\n",
    "                  \n",
    "\n",
    "\n",
    "    def get_user_handles(self):\n",
    "        \"\"\"\n",
    "        Get all the unique user handles associated with this dataframe\n",
    "        \"\"\"\n",
    "        return self.data.user_handle.unique()\n",
    "    \n",
    "    def set_users(self, user_list):\n",
    "        \"\"\"\n",
    "        Use the user_list to define all possible users, not just those associated \n",
    "        with the inheriting classes.  Allows the set arrays to match in size between\n",
    "        classes.\n",
    "        \"\"\"\n",
    "        self.user_list = user_list\n",
    "        try:\n",
    "            self.num_users = len(user_list)\n",
    "        except TypeError:\n",
    "            print('set_users - user_list is empty')\n",
    "            sys.exit()\n",
    "\n",
    "        \n",
    "    def calculate_all_jaccard_distances(self):\n",
    "        self.jaccard_distances = jaccard_distances_array(self.user_data_sets, self.num_users)\n",
    "        \n",
    "    def calculate_handle_jaccard_distances(self, handle):\n",
    "        return jaccard_distances_user(self.user_data_sets, handle, self.num_users)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Assessments(PS_Data):\n",
    "    \"\"\"\n",
    "    Class to store assessment-related information.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_file):\n",
    "        PS_Data.__init__(self, input_file)\n",
    "        \n",
    "        \n",
    "        self.load_data()\n",
    "    \n",
    "        \n",
    "    def load_user_data_set(self):\n",
    "        \"\"\"\n",
    "        Convert the dataframe into a user handle-based array of sets indicating\n",
    "        assessment results.\n",
    "        \"\"\"\n",
    "        print('Assessments - load_user_data_set')\n",
    "        \n",
    "        try:\n",
    "            assessment_set = [set() for x in range(len(self.user_list))]\n",
    "        except TypeError:\n",
    "            print('load_user_data_set - user_list is empty')\n",
    "            sys.exit()\n",
    "\n",
    "        # Tack on _Novice, _Proficient, and _Expert to the assessment names -\n",
    "        # for an Expert use all three, Proficient uses two.              \n",
    "        for index, row in self.data.iterrows():\n",
    "            curr_user = assessment_set[row.user_handle-1]\n",
    "            if (row.user_assessment_score >= 200):\n",
    "                curr_user.add(row.assessment_tag +'_Expert')\n",
    "                curr_user.add(row.assessment_tag + '_Proficient')\n",
    "                curr_user.add(row.assessment_tag +'_Novice')\n",
    "            elif (row.user_assessment_score >= 100):\n",
    "                curr_user.add(row.assessment_tag + '_Proficient')\n",
    "                curr_user.add(row.assessment_tag + '_Novice')\n",
    "            else:\n",
    "                curr_user.add(row.assessment_tag + '_Novice')\n",
    "        self.user_data_sets = assessment_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interests(PS_Data):\n",
    "    \"\"\"\n",
    "    Class to store interest-related information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_file):\n",
    "        \n",
    "        PS_Data.__init__(self, input_file)\n",
    "        \n",
    "        \n",
    "        self.load_data()\n",
    "    \n",
    "    def load_user_data_set(self):\n",
    "        \"\"\"\n",
    "        Convert the dataframe into a user handle based array of sets indicating\n",
    "        interests.\n",
    "        \"\"\"\n",
    "        print('Interests - load_user_data_set')\n",
    "\n",
    "        try:\n",
    "            interest_set = [set() for x in range(len(self.user_list))]\n",
    "        except TypeError:\n",
    "            print('load_user_data_set - user_list is empty')\n",
    "            sys.exit()\n",
    "\n",
    "        for index, row in self.data.iterrows():\n",
    "            curr_user = interest_set[row.user_handle-1]\n",
    "            curr_user.add(row.interest_tag)\n",
    "            \n",
    "        self.user_data_sets = interest_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classes(PS_Data):\n",
    "    \"\"\"\n",
    "    Class to store class-interest related information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_file, tags_file):\n",
    "        PS_Data.__init__(self, input_file)\n",
    "        self.tags_file = tags_file\n",
    "        self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Loading dataframe for Classes is slightly more complex as it also makes use of the \n",
    "        tags file.\n",
    "        \"\"\"\n",
    "        try: \n",
    "            self.data_course_tags = pd.read_csv(self.tags_file)\n",
    "        except FileNotFoundError:\n",
    "            print('load_data - could not find tags file {0}'.format(self.tags_file))\n",
    "            sys.exit() \n",
    "\n",
    "        try:    \n",
    "            self.data  = pd.read_csv(self.input_file)\n",
    "        except FileNotFoundError:\n",
    "            print('load_data - could not find file {0}'.format(self.input_file))\n",
    "            sys.exit() \n",
    "\n",
    "        self.data = self.data.merge(self.data_course_tags[['course_id', 'course_tags']], on=['course_id'])\n",
    "        self.data.drop('view_date', axis=1, inplace=True)\n",
    "        self.data.drop('author_handle', axis=1, inplace=True)\n",
    "        self.data.drop('view_time_seconds', axis=1, inplace=True)\n",
    "        self.data.drop('course_id', axis=1, inplace=True) # Using course_tags for similarity\n",
    "        self.data.drop_duplicates(inplace=True)\n",
    "        \n",
    "        \n",
    "    def load_user_data_set(self):  \n",
    "        \"\"\"\n",
    "        Convert the dataframe into a user handle based array of sets indicating\n",
    "        class interests.\n",
    "        \"\"\"\n",
    "        print('Classes - load_user_data_set')\n",
    "\n",
    "        try:\n",
    "            course_set = [set() for x in range(len(self.user_list))]\n",
    "        except TypeError:\n",
    "            print('load_user_data_set - user_list is empty')\n",
    "            sys.exit()\n",
    "            \n",
    "        # Tack on _Beginner, _Intermediate, and _Advanced to the class names -\n",
    "        # for an Expert use all three, Proficient uses two.\n",
    "\n",
    "        for index, row in self.data.iterrows():\n",
    "            curr_user = course_set[row.user_handle-1]\n",
    "            course_tag = str(row.course_tags)\n",
    "            course_tag = course_tag + '_' \n",
    "            curr_user.add(course_tag + row.level)\n",
    "            if row.level == 'Advanced':\n",
    "                curr_user.add(course_tag + 'Intermediate')\n",
    "                curr_user.add(course_tag + 'Beginner')\n",
    "            elif row.level == 'Intermediate':\n",
    "                curr_user.add(course_tag + 'Beginner')\n",
    "        self.user_data_sets = course_set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sql_table():\n",
    "    \"\"\"\n",
    "    Connect to the database 'dstack.db' and create the distance_matrix table.\n",
    "    Delete the table if it already exists.\n",
    "    \n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect('dstack.db')\n",
    "\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # Create table\n",
    "\n",
    "    c.execute('''DROP TABLE IF EXISTS distance_matrix''')\n",
    "\n",
    "    c.execute('''CREATE TABLE distance_matrix\n",
    "             (src_usr integer, dst_usr integer, distance real)''')\n",
    "\n",
    "    # Commit the changes\n",
    "    conn.commit()\n",
    "\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading user data sets\n",
      "Assessments - load_user_data_set\n",
      "Interests - load_user_data_set\n",
      "Classes - load_user_data_set\n",
      "Calculating Jaccard distances across all axes and storing via sql - 10000 entries\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "125\n",
      "150\n",
      "175\n",
      "200\n",
      "225\n",
      "250\n",
      "275\n",
      "300\n",
      "325\n",
      "350\n",
      "375\n",
      "400\n",
      "425\n",
      "450\n",
      "475\n",
      "500\n",
      "525\n",
      "550\n",
      "575\n",
      "600\n",
      "625\n",
      "650\n",
      "675\n",
      "700\n",
      "725\n",
      "750\n",
      "775\n",
      "800\n",
      "825\n",
      "850\n",
      "875\n",
      "900\n",
      "925\n",
      "950\n",
      "975\n",
      "1000\n",
      "1025\n",
      "1050\n",
      "1075\n",
      "1100\n",
      "1125\n",
      "1150\n",
      "1175\n",
      "1200\n",
      "1225\n",
      "1250\n",
      "1275\n",
      "1300\n",
      "1325\n",
      "1350\n",
      "1375\n",
      "1400\n",
      "1425\n",
      "1450\n",
      "1475\n",
      "1500\n",
      "1525\n",
      "1550\n",
      "1575\n",
      "1600\n",
      "1625\n",
      "1650\n",
      "1675\n",
      "1700\n",
      "1725\n",
      "1750\n",
      "1775\n",
      "1800\n",
      "1825\n",
      "1850\n",
      "1875\n",
      "1900\n",
      "1925\n",
      "1950\n",
      "1975\n",
      "2000\n",
      "2025\n",
      "2050\n",
      "2075\n",
      "2100\n",
      "2125\n",
      "2150\n",
      "2175\n",
      "2200\n",
      "2225\n",
      "2250\n",
      "2275\n",
      "2300\n",
      "2325\n",
      "2350\n",
      "2375\n",
      "2400\n",
      "2425\n",
      "2450\n",
      "2475\n",
      "2500\n",
      "2525\n",
      "2550\n",
      "2575\n",
      "2600\n",
      "2625\n",
      "2650\n",
      "2675\n",
      "2700\n",
      "2725\n",
      "2750\n",
      "2775\n",
      "2800\n",
      "2825\n",
      "2850\n",
      "2875\n",
      "2900\n",
      "2925\n",
      "2950\n",
      "2975\n",
      "3000\n",
      "3025\n",
      "3050\n",
      "3075\n",
      "3100\n",
      "3125\n",
      "3150\n",
      "3175\n",
      "3200\n",
      "3225\n",
      "3250\n",
      "3275\n",
      "3300\n",
      "3325\n",
      "3350\n",
      "3375\n",
      "3400\n",
      "3425\n",
      "3450\n",
      "3475\n",
      "3500\n",
      "3525\n",
      "3550\n",
      "3575\n",
      "3600\n",
      "3625\n",
      "3650\n",
      "3675\n",
      "3700\n",
      "3725\n",
      "3750\n",
      "3775\n",
      "3800\n",
      "3825\n",
      "3850\n",
      "3875\n",
      "3900\n",
      "3925\n",
      "3950\n",
      "3975\n",
      "4000\n",
      "4025\n",
      "4050\n",
      "4075\n",
      "4100\n",
      "4125\n",
      "4150\n",
      "4175\n",
      "4200\n",
      "4225\n",
      "4250\n",
      "4275\n",
      "4300\n",
      "4325\n",
      "4350\n",
      "4375\n",
      "4400\n",
      "4425\n",
      "4450\n",
      "4475\n",
      "4500\n",
      "4525\n",
      "4550\n",
      "4575\n",
      "4600\n",
      "4625\n",
      "4650\n",
      "4675\n",
      "4700\n",
      "4725\n",
      "4750\n",
      "4775\n",
      "4800\n",
      "4825\n",
      "4850\n",
      "4875\n",
      "4900\n",
      "4925\n",
      "4950\n",
      "4975\n",
      "5000\n",
      "5025\n",
      "5050\n",
      "5075\n",
      "5100\n",
      "5125\n",
      "5150\n",
      "5175\n",
      "5200\n",
      "5225\n",
      "5250\n",
      "5275\n",
      "5300\n",
      "5325\n",
      "5350\n",
      "5375\n",
      "5400\n",
      "5425\n",
      "5450\n",
      "5475\n",
      "5500\n",
      "5525\n",
      "5550\n",
      "5575\n",
      "5600\n",
      "5625\n",
      "5650\n",
      "5675\n",
      "5700\n",
      "5725\n",
      "5750\n",
      "5775\n",
      "5800\n",
      "5825\n",
      "5850\n",
      "5875\n",
      "5900\n",
      "5925\n",
      "5950\n",
      "5975\n",
      "6000\n",
      "6025\n",
      "6050\n",
      "6075\n",
      "6100\n",
      "6125\n",
      "6150\n",
      "6175\n",
      "6200\n",
      "6225\n",
      "6250\n",
      "6275\n",
      "6300\n",
      "6325\n",
      "6350\n",
      "6375\n",
      "6400\n",
      "6425\n",
      "6450\n",
      "6475\n",
      "6500\n",
      "6525\n",
      "6550\n",
      "6575\n",
      "6600\n",
      "6625\n",
      "6650\n",
      "6675\n",
      "6700\n",
      "6725\n",
      "6750\n",
      "6775\n",
      "6800\n",
      "6825\n",
      "6850\n",
      "6875\n",
      "6900\n",
      "6925\n",
      "6950\n",
      "6975\n",
      "7000\n",
      "7025\n",
      "7050\n",
      "7075\n",
      "7100\n",
      "7125\n",
      "7150\n",
      "7175\n",
      "7200\n",
      "7225\n",
      "7250\n",
      "7275\n",
      "7300\n",
      "7325\n",
      "7350\n",
      "7375\n",
      "7400\n",
      "7425\n",
      "7450\n",
      "7475\n",
      "7500\n",
      "7525\n",
      "7550\n",
      "7575\n",
      "7600\n",
      "7625\n",
      "7650\n",
      "7675\n",
      "7700\n",
      "7725\n",
      "7750\n",
      "7775\n",
      "7800\n",
      "7825\n",
      "7850\n",
      "7875\n",
      "7900\n",
      "7925\n",
      "7950\n",
      "7975\n",
      "8000\n",
      "8025\n",
      "8050\n",
      "8075\n",
      "8100\n",
      "8125\n",
      "8150\n",
      "8175\n",
      "8200\n",
      "8225\n",
      "8250\n",
      "8275\n",
      "8300\n",
      "8325\n",
      "8350\n",
      "8375\n",
      "8400\n",
      "8425\n",
      "8450\n",
      "8475\n",
      "8500\n",
      "8525\n",
      "8550\n",
      "8575\n",
      "8600\n",
      "8625\n",
      "8650\n",
      "8675\n",
      "8700\n",
      "8725\n",
      "8750\n",
      "8775\n",
      "8800\n",
      "8825\n",
      "8850\n",
      "8875\n",
      "8900\n",
      "8925\n",
      "8950\n",
      "8975\n",
      "9000\n",
      "9025\n",
      "9050\n",
      "9075\n",
      "9100\n",
      "9125\n",
      "9150\n",
      "9175\n",
      "9200\n",
      "9225\n",
      "9250\n",
      "9275\n",
      "9300\n",
      "9325\n",
      "9350\n",
      "9375\n",
      "9400\n",
      "9425\n",
      "9450\n",
      "9475\n",
      "9500\n",
      "9525\n",
      "9550\n",
      "9575\n",
      "9600\n",
      "9625\n",
      "9650\n",
      "9675\n",
      "9700\n",
      "9725\n",
      "9750\n",
      "9775\n",
      "9800\n",
      "9825\n",
      "9850\n",
      "9875\n",
      "9900\n",
      "9925\n",
      "9950\n",
      "9975\n",
      "10000\n",
      "Done. Data stored via sql.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    Assessments_Obj = Assessments('data_files_ml_engineer/user_assessment_scores.csv')\n",
    "    Interests_Obj = Interests('data_files_ml_engineer/user_interests.csv')\n",
    "    Classes_Obj = Classes('data_files_ml_engineer/user_course_views.csv', 'data_files_ml_engineer/course_tags.csv')\n",
    "\n",
    "\n",
    "    # Make a list of the three objects\n",
    "    Obj_List = [Assessments_Obj, Interests_Obj, Classes_Obj]\n",
    "\n",
    "\n",
    "    # Get all our users - as it turns out there are 10,000 ranging from 1 to 10,000.\n",
    "    # This code will take advantage of that - in a more generic problem we would need\n",
    "    # to accept the possibility of gaps.\n",
    "    user_lists = [obj.get_user_handles() for obj in Obj_List]\n",
    "    user_list = np.unique([item for sublist in user_lists for item in sublist])\n",
    "\n",
    "    # Tell each object about the users\n",
    "    [obj.set_users(user_list) for obj in Obj_List]\n",
    "\n",
    "    # For each object translate the data in its dataframe into an array of sets\n",
    "    print('Loading user data sets')\n",
    "    [obj.load_user_data_set() for obj in Obj_List]\n",
    "\n",
    "    # Create a sql table and store Jaccard distances in it.\n",
    "    create_sql_table()\n",
    "    print('Calculating Jaccard distances across all axes and storing via sql - {0} entries'.format(len(user_list)))\n",
    "    for i in range(1, len(user_list) + 1):\n",
    "        if (i%25) == 0:\n",
    "            print (i)\n",
    "        sql_3_axis_distances_for_one_handle(Assessments_Obj, Interests_Obj, Classes_Obj, i)\n",
    "    print('Done. Data stored via sql.')\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some standalone code to do quick verification of data\n",
    "\n",
    "conn = sqlite3.connect('dstack.db')\n",
    "  \n",
    "c = conn.cursor()\n",
    "c.execute(\"\"\"SELECT dst_usr, distance FROM distance_matrix where src_usr = 9999 and src_usr != dst_usr ORDER BY distance LIMIT 10\"\"\")\n",
    "        \n",
    "rows = c.fetchall()\n",
    "        \n",
    "        \n",
    "# Save (commit) the changes\n",
    "conn.commit()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "user_dict =  {k:v for k, v in rows}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1562: 0.0,\n",
       " 5238: 0.0,\n",
       " 6303: 0.0,\n",
       " 4484: 0.5,\n",
       " 4432: 0.5196152422706632,\n",
       " 906: 0.5204164998665332,\n",
       " 2200: 0.5443310539518175,\n",
       " 3605: 0.5446711546122731,\n",
       " 5071: 0.5608545472127794,\n",
       " 453: 0.5773502691896258}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
